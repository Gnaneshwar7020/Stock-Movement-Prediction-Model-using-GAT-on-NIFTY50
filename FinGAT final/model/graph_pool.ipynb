{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ee8db5-4126-42a1-a174-b23d3cabe806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, SAGPooling,global_mean_pool , global_max_pool \n",
    "\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self,time_step,dim):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.attention_matrix = nn.Linear(time_step, time_step)\n",
    "\n",
    "    def forward(self, inputs):  \n",
    "        inputs_t = torch.transpose(inputs,2,1) # (B, T, C) -> (B, C, T)\n",
    "        attention_weight = self.attention_matrix(inputs_t) # (B, C ,T) * (B, T ,T) -> (B, C, T)\n",
    "        attention_probs = F.softmax(attention_weight,dim=-1) # softmax along the last dim which means every row of C will have sum as 1 \n",
    "        attention_probs = torch.transpose(attention_probs,2,1) # (B, C, T) -> (B, T, C) now every col has sum = 1\n",
    "        attention_vec = torch.mul(attention_probs, inputs) # multiplying the softmax matrix with input\n",
    "        attention_vec = torch.sum(attention_vec,dim=1) # sum along the second dimension, (B, C) will be the final shape\n",
    "        return attention_vec, attention_probs #(B, C) for att vec, (B,T,C) for att probs\n",
    "\n",
    "class SequenceEncoder(nn.Module):\n",
    "    def __init__(self,input_dim,time_step,hidden_dim):\n",
    "        super(SequenceEncoder, self).__init__()\n",
    "        self.encoder = nn.GRU(input_size=input_dim,hidden_size=hidden_dim,num_layers=1,batch_first=True)\n",
    "        self.attention_block = AttentionBlock(time_step,hidden_dim) \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.dim = hidden_dim\n",
    "    # I think C is equal to hidden dim in this case\n",
    "    def forward(self,seq):\n",
    "        '''\n",
    "        inp : torch.tensor (batch,time_step,input_dim)\n",
    "        '''\n",
    "        seq_vector,_ = self.encoder(seq)    # (B, T, hidden_dim)\n",
    "        seq_vector = self.dropout(seq_vector)\n",
    "        attention_vec, _ = self.attention_block(seq_vector) # (B, hidden_dim)\n",
    "        attention_vec = attention_vec.view(-1,1,self.dim) # prepare for concat   i think this transforms to (B, 1, hidden_dim)\n",
    "        return attention_vec\n",
    "\n",
    "\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self,input_dim,time_step,hidden_dim):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.encoder = nn.GRU(input_size=input_dim,hidden_size=hidden_dim,num_layers=1,batch_first=True)\n",
    "        self.gat = GATConv(hidden_dim,hidden_dim)\n",
    "        self.dim = hidden_dim\n",
    "    \n",
    "    def forward(self,seq,edge_index):\n",
    "        '''\n",
    "        inp : torch.tensor (batch,time_step,input_dim)\n",
    "        '''\n",
    "        seq_vector,_ = self.encoder(seq)\n",
    "        seq_vector = seq_vector[:,-1,:]\n",
    "        attention_vec = self.gat(seq_vector,edge_index)\n",
    "        return seq_vector, attention_vec\n",
    "\n",
    "\n",
    "class CategoricalGraph(nn.Module):\n",
    "    def __init__(self,input_dim,time_step,hidden_dim,inner_edge,outer_edge,input_num,device):\n",
    "        super(CategoricalGraph, self).__init__()\n",
    "\n",
    "        # basic parameters\n",
    "        self.dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.time_step = time_step\n",
    "        self.inner_edge = inner_edge\n",
    "        self.outer_edge = outer_edge\n",
    "        self.input_num = input_num\n",
    "        self.device = device\n",
    "\n",
    "        # hidden layers\n",
    "        self.encoder_list = nn.ModuleList([SequenceEncoder(input_dim,time_step,hidden_dim) for _ in range(input_num)])\n",
    "        self.cat_gat = GATConv(hidden_dim,hidden_dim)\n",
    "        self.weekly_attention = AttentionBlock(input_num,hidden_dim)\n",
    "        self.fusion = nn.Linear(hidden_dim*2,hidden_dim)\n",
    "\n",
    "        # output layer \n",
    "        self.reg_layer = nn.Linear(hidden_dim,1)\n",
    "        self.cls_layer = nn.Linear(hidden_dim,1)\n",
    "\n",
    "    def forward(self,weekly_batch):\n",
    "        # x has shape (category_num, stocks_num, time_step, dim)\n",
    "        weekly_embedding = self.encoder_list[0](weekly_batch[0].view(-1,self.time_step,self.input_dim)) # (100,1,dim)  // concatenation\n",
    "\n",
    "        # calculate embeddings for the rest of weeks\n",
    "        for week_idx in range(1,self.input_num):\n",
    "            weekly_inp = weekly_batch[week_idx] # (category_num, stocks_num, time_step, dim)\n",
    "            weekly_inp = weekly_inp.view(-1,self.time_step,self.input_dim) # reshape for faster training \n",
    "            week_stock_embedding = self.encoder_list[week_idx](weekly_inp) # (100,1,dim)\n",
    "            weekly_embedding = torch.cat((weekly_embedding,week_stock_embedding),dim=1)\n",
    "\n",
    "        # merge weeks \n",
    "        weekly_att_vector,_ = self.weekly_attention(weekly_embedding) # (100,dim)\n",
    "        # weekly_att_vector = weekly_att_vector.view(5,20,-1)\n",
    "        # category_vectors,_ = torch.max(weekly_att_vector,dim=1)\n",
    "\n",
    "        # use category graph attention \n",
    "        category_vectors = self.cat_gat(weekly_att_vector,self.inner_edge) # (5,dim)\n",
    "        # category_vectors = category_vectors.unsqueeze(1).expand(-1,20,-1)\n",
    "\n",
    "        # fusion \n",
    "        fusion_vec = torch.cat((weekly_att_vector,category_vectors),dim=-1)\n",
    "        fusion_vec = torch.relu(self.fusion(fusion_vec))\n",
    "\n",
    "        # output\n",
    "        reg_output = self.reg_layer(fusion_vec)\n",
    "        reg_output = torch.flatten(reg_output)\n",
    "        cls_output = torch.sigmoid(self.cls_layer(fusion_vec))\n",
    "        cls_output = torch.flatten(cls_output)\n",
    "\n",
    "        return reg_output, cls_output\n",
    "\n",
    "    def predict_toprank(self,test_data,device,top_k=5):\n",
    "        y_pred_all = []\n",
    "        test_w2,test_w3,test_w4 = test_data\n",
    "        for idx,_ in enumerate(test_w2):\n",
    "            batch_x2,batch_x3,batch_x4 = test_w2[idx].to(self.device),\\\n",
    "                                        test_w3[idx].to(self.device),\\\n",
    "                                        test_w4[idx].to(self.device)\n",
    "            batch_weekly = [batch_x2,batch_x3,batch_x4]\n",
    "            pred = self.forward(batch_weekly)[0].cpu().detach().numpy()\n",
    "            y_pred_all.extend(pred.tolist())\n",
    "        return y_pred_all    \n",
    "\n",
    "\n",
    "\n",
    "class CategoricalGraphAtt(nn.Module):\n",
    "    def __init__(self,input_dim,time_step,hidden_dim,inner_edge,outer_edge,no_of_weeks_to_look_back,use_gru,device):\n",
    "        super(CategoricalGraphAtt, self).__init__()\n",
    "\n",
    "        # basic parameters\n",
    "        self.dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.time_step = time_step\n",
    "        self.inner_edge = inner_edge\n",
    "        self.outer_edge = outer_edge\n",
    "        self.no_of_weeks_to_look_back = no_of_weeks_to_look_back\n",
    "        self.use_gru = use_gru\n",
    "        self.device = device\n",
    "\n",
    "        # hidden layers\n",
    "        self.pool_attention = AttentionBlock(10,hidden_dim)\n",
    "        if self.use_gru:\n",
    "            self.weekly_encoder = nn.GRU(hidden_dim,hidden_dim)\n",
    "        self.encoder_list = nn.ModuleList([SequenceEncoder(input_dim,time_step,hidden_dim) for _ in range(no_of_weeks_to_look_back)]) \n",
    "        self.cat_gat = GATConv(hidden_dim,hidden_dim)\n",
    "        self.inner_gat = GATConv(hidden_dim,hidden_dim)\n",
    "        self.weekly_attention = AttentionBlock(no_of_weeks_to_look_back,hidden_dim)\n",
    "        self.fusion = nn.Linear(hidden_dim*3,hidden_dim)\n",
    "\n",
    "        # output layer \n",
    "        self.reg_layer = nn.Linear(hidden_dim,1)\n",
    "        self.cls_layer = nn.Linear(hidden_dim,1)\n",
    "\n",
    "    def forward(self,weekly_batch):\n",
    "        print(\"0\")\n",
    "        # x has shape (category_num, stocks_num, time_step, dim)\n",
    "        # Taiwan has (5 sectors, and 100 stocks, T, C)  i think the time step is 1 all throughout\n",
    "        weekly_embedding = self.encoder_list[0](weekly_batch[0].view(-1,self.time_step,self.input_dim)) # (100,1,dim)\n",
    "        print(\"1\")\n",
    "        # calculate embeddings for the rest of weeks\n",
    "        for week_idx in range(1,self.no_of_weeks_to_look_back):\n",
    "            print(\"2.1\")\n",
    "            weekly_inp = weekly_batch[week_idx] # (category_num, stocks_num, time_step, dim)\n",
    "            print(\"2.2\")\n",
    "            weekly_inp = weekly_inp.view(-1,self.time_step,self.input_dim) # reshape for faster training \n",
    "            print(\"2.3\")\n",
    "            week_stock_embedding = self.encoder_list[week_idx](weekly_inp) # (100,1,dim)\n",
    "            print(\"2.4\")\n",
    "            weekly_embedding = torch.cat((weekly_embedding,week_stock_embedding),dim=1)\n",
    "            print(\"2.5\")\n",
    "        print(\"3\")\n",
    "        # merge weeks \n",
    "        if self.use_gru:\n",
    "            weekly_embedding,_ = self.weekly_encoder(weekly_embedding)\n",
    "        weekly_att_vector,_ = self.weekly_attention(weekly_embedding) # (100,dim)\n",
    "        print(\"4\")\n",
    "\n",
    "        # inner graph interaction \n",
    "        inner_graph_embedding = self.inner_gat(weekly_att_vector,self.inner_edge)\n",
    "        print(\"5\")\n",
    "        inner_graph_embedding = inner_graph_embedding.view(5,10,-1)\n",
    "        print(\"6\")\n",
    "\n",
    "        # pooling \n",
    "        weekly_att_vector = weekly_att_vector.view(5,10,-1)\n",
    "        print(\"7\")\n",
    "        category_vectors,_ =  self.pool_attention(weekly_att_vector) #torch.max(weekly_att_vector,dim=1)\n",
    "        print(\"8\")\n",
    "\n",
    "        # use category graph attention \n",
    "        category_vectors = self.cat_gat(category_vectors,self.outer_edge) # (5,dim)\n",
    "        print(\"9\")\n",
    "        category_vectors = category_vectors.unsqueeze(1).expand(-1,10,-1)\n",
    "        print(\"10\")\n",
    "\n",
    "        # fusion \n",
    "        fusion_vec = torch.cat((weekly_att_vector,category_vectors,inner_graph_embedding),dim=-1)  # I think its the Ti(A), Ti(G), Ti(πc)\n",
    "        print(\"11\")\n",
    "        fusion_vec = torch.relu(self.fusion(fusion_vec))\n",
    "        print(\"12\")\n",
    "\n",
    "        # output\n",
    "        reg_output = self.reg_layer(fusion_vec)\n",
    "        print(\"13\")\n",
    "        reg_output = torch.flatten(reg_output)\n",
    "        print(\"14\")\n",
    "        cls_output = torch.sigmoid(self.cls_layer(fusion_vec))\n",
    "        print(\"15\")\n",
    "        cls_output = torch.flatten(cls_output)\n",
    "        print(\"16\")\n",
    "\n",
    "        return reg_output, cls_output\n",
    "\n",
    "    def predict_toprank(self,test_data,device,top_k=5):\n",
    "        print(\"17\")\n",
    "        y_pred_all_reg, y_pred_all_cls = [], []\n",
    "        test_w1,test_w2,test_w3,test_w4 = test_data\n",
    "        for idx,_ in enumerate(test_w2):\n",
    "            print(\"18\")\n",
    "            batch_x1,batch_x2,batch_x3,batch_x4 = test_w1[idx].to(self.device), \\\n",
    "                                        test_w2[idx].to(self.device),\\\n",
    "                                        test_w3[idx].to(self.device),\\\n",
    "                                        test_w4[idx].to(self.device)\n",
    "            print(\"19\")\n",
    "            batch_weekly = [batch_x1,batch_x2,batch_x3,batch_x4][-self.no_of_weeks_to_look_back:]\n",
    "            print(\"20\")\n",
    "            pred_reg, pred_cls = self.forward(batch_weekly)\n",
    "            print(\"21\")\n",
    "            pred_reg, pred_cls = pred_reg.cpu().detach().numpy(), pred_cls.cpu().detach().numpy()\n",
    "            print(\"22\")\n",
    "            y_pred_all_reg.extend(pred_reg.tolist())\n",
    "            print(\"23\")\n",
    "            y_pred_all_cls.extend(pred_cls.tolist())\n",
    "            print(\"24\")\n",
    "        return y_pred_all_reg, y_pred_all_cls\n",
    "\n",
    "\n",
    "\n",
    "class CategoricalGraphPool(nn.Module):\n",
    "    def __init__(self,input_dim,time_step,hidden_dim,inner_edge,inner20_edge,outer_edge,input_num,use_gru,device):\n",
    "        super(CategoricalGraphPool, self).__init__()\n",
    "\n",
    "        # basic parameters\n",
    "        self.dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.time_step = time_step\n",
    "        self.inner_edge = inner_edge\n",
    "        self.inner20_edge = inner20_edge\n",
    "        self.outer_edge = outer_edge\n",
    "        self.input_num = input_num\n",
    "        self.use_gru = use_gru\n",
    "        self.device = device\n",
    "\n",
    "        # hidden layers\n",
    "        self.pool_attention = AttentionBlock(20,hidden_dim)\n",
    "        if self.use_gru:\n",
    "           self.weekly_encoder = nn.GRU(hidden_dim,hidden_dim)\n",
    "        self.encoder_list = nn.ModuleList([SequenceEncoder(input_dim,time_step,hidden_dim) for _ in range(input_num)])\n",
    "        self.cat_gat = GATConv(hidden_dim*2,hidden_dim)\n",
    "        self.inner_gat = GATConv(hidden_dim,hidden_dim)\n",
    "        self.pooling_gcn = SAGPooling(hidden_dim,ratio=0.5)\n",
    "        self.weekly_attention = AttentionBlock(input_num,hidden_dim)\n",
    "        self.fusion = nn.Linear(hidden_dim*3,hidden_dim)\n",
    "\n",
    "        # output layer \n",
    "        self.reg_layer = nn.Linear(hidden_dim,1)\n",
    "        self.cls_layer = nn.Linear(hidden_dim,1)\n",
    "\n",
    "    def forward(self,weekly_batch):\n",
    "        # x has shape (category_num, stocks_num, time_step, dim)\n",
    "        weekly_embedding = self.encoder_list[0](weekly_batch[0].view(-1,self.time_step,self.input_dim)) # (100,1,dim)\n",
    "\n",
    "        # calculate embeddings for the rest of weeks\n",
    "        for week_idx in range(1,self.input_num):\n",
    "            weekly_inp = weekly_batch[week_idx] # (category_num, stocks_num, time_step, dim)\n",
    "            weekly_inp = weekly_inp.view(-1,self.time_step,self.input_dim) # reshape for faster training \n",
    "            week_stock_embedding = self.encoder_list[week_idx](weekly_inp) # (100,1,dim)\n",
    "            weekly_embedding = torch.cat((weekly_embedding,week_stock_embedding),dim=1)\n",
    "\n",
    "        # merge weeks \n",
    "        if self.use_gru:\n",
    "            weekly_embedding,_ = self.weekly_encoder(weekly_embedding)\n",
    "        weekly_att_vector,_ = self.weekly_attention(weekly_embedding) # (100,dim)\n",
    "\n",
    "        # inner graph interaction \n",
    "        inner_graph_embedding = self.inner_gat(weekly_att_vector,self.inner_edge)\n",
    "        inner_graph_embedding = inner_graph_embedding.view(5,20,-1)\n",
    "\n",
    "        # pooling \n",
    "        weekly_att_vector = weekly_att_vector.view(5,20,-1)\n",
    "        cat_embdding, _, _, batch, _, _ = self.pooling_gcn(weekly_att_vector[0],self.inner20_edge)\n",
    "        cat_embdding = torch.cat([global_max_pool(cat_embdding, batch), global_mean_pool(cat_embdding, batch)], dim=-1)\n",
    "        cat_embdding = cat_embdding.view(1,-1)\n",
    "\n",
    "        for cat_idx in range(1,5):\n",
    "            topk_embedding, _, _, batch, _, _ = self.pooling_gcn(weekly_att_vector[cat_idx],self.inner20_edge)\n",
    "            topk_embedding = torch.cat([global_max_pool(topk_embedding, batch), global_mean_pool(topk_embedding, batch)], dim=-1)\n",
    "            topk_embedding = topk_embedding.view(1,-1)\n",
    "            cat_embdding = torch.cat((cat_embdding,topk_embedding),dim=0)\n",
    "\n",
    "\n",
    "        # use category graph attention \n",
    "        category_vectors = self.cat_gat(cat_embdding,self.outer_edge) # (5,dim)\n",
    "        category_vectors = category_vectors.unsqueeze(1).expand(-1,20,-1)\n",
    "\n",
    "        # fusion \n",
    "        fusion_vec = torch.cat((weekly_att_vector,category_vectors,inner_graph_embedding),dim=-1)\n",
    "        fusion_vec = torch.relu(self.fusion(fusion_vec))\n",
    "\n",
    "        # output\n",
    "        reg_output = self.reg_layer(fusion_vec)\n",
    "        reg_output = torch.flatten(reg_output)\n",
    "        cls_output = torch.sigmoid(self.cls_layer(fusion_vec))\n",
    "        cls_output = torch.flatten(cls_output)\n",
    "\n",
    "        return reg_output, cls_output\n",
    "\n",
    "    def predict_toprank(self,test_data,device,top_k=5):\n",
    "        y_pred_all = []\n",
    "        test_w1,test_w2,test_w3,test_w4 = test_data\n",
    "        for idx,_ in enumerate(test_w2):\n",
    "            batch_x1,batch_x2,batch_x3,batch_x4 = test_w1[idx].to(self.device), \\\n",
    "                                        test_w2[idx].to(self.device),\\\n",
    "                                        test_w3[idx].to(self.device),\\\n",
    "                                        test_w4[idx].to(self.device)\n",
    "            batch_weekly = [batch_x1,batch_x2,batch_x3,batch_x4][-self.input_num:]\n",
    "            pred = self.forward(batch_weekly)[0].cpu().detach().numpy()\n",
    "            y_pred_all.extend(pred.tolist())\n",
    "        return y_pred_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528c77b-21f8-4f30-a26f-41a2929e39ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
